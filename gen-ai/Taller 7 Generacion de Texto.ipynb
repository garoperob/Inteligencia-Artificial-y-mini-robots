{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ThE5wn5orPCL"
   },
   "source": [
    "Generación de Texto con GPT-3: Realizar ejercicios de generación de texto\n",
    "utilizando una API basada en GPT-3. Pueden generar historias, poemas o\n",
    "respuestas a preguntas específicas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dw4j0vU_rO1E"
   },
   "source": [
    "Este código es un generador de texto que utiliza el modelo gratuito EleutherAI/gpt-neo-1.3B de Hugging Face para crear historias, poemas o responder preguntas a partir de un texto de entrada proporcionado por el usuario. Primero, carga el modelo y el tokenizador necesarios para procesar el texto, configurando correctamente los parámetros para evitar errores. Luego, a través de la función generar_texto(), se genera un contenido basado en el tipo seleccionado por el usuario, con un límite de palabras definido para evitar respuestas demasiado largas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 88799,
     "status": "ok",
     "timestamp": 1741066687428,
     "user": {
      "displayName": "Steven Santiago Guzman Bernal",
      "userId": "12543218635228548930"
     },
     "user_tz": 300
    },
    "id": "3jgh2ogypDs9",
    "outputId": "f6017af6-2b8e-4249-b188-ad45832c106b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecciona el tipo de texto que deseas generar:\n",
      "1. Historia\n",
      "2. Poema\n",
      "3. Respuesta a una pregunta específica\n",
      "Elige una opción (1/2/3): 3\n",
      "Ingresa el tema o pregunta: Cuanto tarda en llegar la luz del sol a la tierra?\n",
      "\n",
      "Texto generado:\n",
      "Cuanto tarda en llegar la luz del sol a la tierra?\n",
      "\n",
      "Aún tiene pocos años. Pero esa pocidad está ahí, en una luna llena... en la tierra roja y apestosa.\n",
      "\n",
      "Los niños vienen a la bola. Alargada, alcanza la cabeza. Siente el sol. Se levanta en silencio. Susurra algo. Más dificultades... El olor de la tierra.\n",
      "\n",
      "Déjelo esperar porque aún puedes estar un poco más\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Cargar modelo y tokenizador\n",
    "modelo = \"EleutherAI/gpt-neo-1.3B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(modelo)\n",
    "modelo_generacion = AutoModelForCausalLM.from_pretrained(modelo)\n",
    "\n",
    "generador = pipeline(\"text-generation\", model=modelo_generacion, tokenizer=tokenizer)\n",
    "\n",
    "def generar_texto(prompt, tipo=\"historia\", max_tokens=150):\n",
    "    \"\"\"\n",
    "    Genera texto utilizando un modelo gratuito de Hugging Face.\n",
    "    :param prompt: Entrada de texto para la generación.\n",
    "    :param tipo: Tipo de texto a generar (historia, poema, respuesta).\n",
    "    :param max_tokens: Máximo número de tokens en la respuesta.\n",
    "    :return: Texto generado.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = generador(\n",
    "            prompt,\n",
    "            max_length=max_tokens,\n",
    "            do_sample=True,\n",
    "            truncation=True,\n",
    "            pad_token_id=tokenizer.eos_token_id  # Evita el warning de pad_token_id\n",
    "        )\n",
    "        return response[0][\"generated_text\"].strip()\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Selecciona el tipo de texto que deseas generar:\")\n",
    "    print(\"1. Historia\")\n",
    "    print(\"2. Poema\")\n",
    "    print(\"3. Respuesta a una pregunta específica\")\n",
    "    opcion = input(\"Elige una opción (1/2/3): \")\n",
    "\n",
    "    tipos = {\"1\": \"historia\", \"2\": \"poema\", \"3\": \"respuesta\"}\n",
    "    tipo_texto = tipos.get(opcion, \"historia\")\n",
    "\n",
    "    prompt = input(\"Ingresa el tema o pregunta: \")\n",
    "    resultado = generar_texto(prompt, tipo=tipo_texto)\n",
    "    print(\"\\nTexto generado:\")\n",
    "    print(resultado)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyO7fsLLXXx6Pj4RLpXYDY4F",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
