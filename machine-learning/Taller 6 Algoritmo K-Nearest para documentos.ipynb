{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP7jhVl4+5/RGqo8lo+LavF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Busque un ejemplo se utilice un algoritmo de K- Nearest, o árboles de decisión.\n","\n"],"metadata":{"id":"iBfBQDuNFVOQ"}},{"cell_type":"markdown","source":["En este caso en empleo un Algoritmo K-Nearest para la clasificacion de documentos con el dataset de 20newgroups."],"metadata":{"id":"8eyCnnmOF36g"}},{"cell_type":"code","source":["from sklearn.datasets import fetch_20newsgroups\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.model_selection import train_test_split\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.metrics import accuracy_score\n","from sklearn.decomposition import TruncatedSVD\n","import time\n","import requests\n","from bs4 import BeautifulSoup\n","import random\n","\n","# Cargar el conjunto de datos\n","print(\"Cargando el conjunto de datos...\")\n","categories = ['sci.space', 'comp.graphics', 'rec.sport.hockey', 'talk.politics.mideast']\n","newsgroups = fetch_20newsgroups(subset='all', categories=categories, remove=('headers', 'footers', 'quotes'))\n","print(f'Datos cargados: {len(newsgroups.data)} documentos')\n","\n","# Convertir los textos en vectores TF-IDF\n","print(\"Transformando los textos en vectores TF-IDF...\")\n","vectorizer = TfidfVectorizer(stop_words='english', max_features=20000, ngram_range=(1,3))\n","X = vectorizer.fit_transform(newsgroups.data)\n","y = newsgroups.target\n","print(f'Tamaño de la matriz de características: {X.shape}')\n","\n","# Reducir dimensionalidad con LSA\n","print(\"Reduciendo la dimensionalidad con LSA...\")\n","svd = TruncatedSVD(n_components=500)\n","X_reduced = svd.fit_transform(X)\n","print(f'Tamaño de la matriz reducida: {X_reduced.shape}')\n","\n","# Dividir en conjuntos de entrenamiento y prueba\n","print(\"Dividiendo datos en entrenamiento y prueba...\")\n","X_train, X_test, y_train, y_test = train_test_split(X_reduced, y, test_size=0.2, random_state=42)\n","print(f'Tamaño del conjunto de entrenamiento: {X_train.shape[0]} muestras')\n","print(f'Tamaño del conjunto de prueba: {X_test.shape[0]} muestras')\n","\n","# Modelo KNN\n","print(\"Entrenando modelo KNN...\")\n","start_time = time.time()\n","knn = KNeighborsClassifier(n_neighbors=3, weights='distance', metric='cosine')\n","knn.fit(X_train, y_train)\n","train_time = time.time() - start_time\n","print(f'Modelo entrenado en {train_time:.2f} segundos')\n","\n","# Predicción y evaluación\n","print(\"Realizando predicciones...\")\n","y_pred = knn.predict(X_test)\n","accuracy = accuracy_score(y_test, y_pred)\n","print(f'Precisión de KNN en clasificación de textos: {accuracy:.2f}')\n","\n","# Probar con un documento de internet\n","def fetch_web_text(url):\n","    \"\"\"Descarga y extrae texto de una página web.\"\"\"\n","    response = requests.get(url)\n","    if response.status_code == 200:\n","        soup = BeautifulSoup(response.text, 'html.parser')\n","        paragraphs = soup.find_all('p')\n","        text = ' '.join([p.get_text() for p in paragraphs])\n","        return text\n","    else:\n","        return None\n","\n","# URL de prueba\n","url = input(\"Ingrese la URL de un documento para clasificar: \")\n","web_text = fetch_web_text(url)\n","\n","if web_text:\n","    web_vector = vectorizer.transform([web_text])\n","    web_vector_reduced = svd.transform(web_vector)\n","    prediction = knn.predict(web_vector_reduced)\n","    print(f'El documento ingresado pertenece a la categoría: {categories[prediction[0]]}')\n","else:\n","    print(\"No se pudo obtener el texto de la URL proporcionada.\")\n","\n","# Probar con un documento del propio dataset\n","def test_with_dataset_sample():\n","    \"\"\"Selecciona un documento aleatorio del conjunto de prueba y lo clasifica.\"\"\"\n","    random_index = random.randint(0, len(X_test) - 1)\n","    sample_text = newsgroups.data[random_index]\n","    sample_vector = vectorizer.transform([sample_text])\n","    sample_vector_reduced = svd.transform(sample_vector)\n","    prediction = knn.predict(sample_vector_reduced)\n","    print(\"\\nPrueba con un documento del dataset:\")\n","    print(f'Texto de prueba: {sample_text[:500]}...')\n","    print(f'Categoría real: {categories[y[random_index]]}')\n","    print(f'Categoría predicha: {categories[prediction[0]]}')\n","\n","test_with_dataset_sample()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xm7AFXnUWvsH","executionInfo":{"status":"ok","timestamp":1741196196425,"user_tz":300,"elapsed":31588,"user":{"displayName":"Steven Santiago Guzman Bernal","userId":"12543218635228548930"}},"outputId":"8f98082b-68ec-43e5-8983-b95d5e0c650f"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Cargando el conjunto de datos...\n","Datos cargados: 3899 documentos\n","Transformando los textos en vectores TF-IDF...\n","Tamaño de la matriz de características: (3899, 20000)\n","Reduciendo la dimensionalidad con LSA...\n","Tamaño de la matriz reducida: (3899, 500)\n","Dividiendo datos en entrenamiento y prueba...\n","Tamaño del conjunto de entrenamiento: 3119 muestras\n","Tamaño del conjunto de prueba: 780 muestras\n","Entrenando modelo KNN...\n","Modelo entrenado en 0.00 segundos\n","Realizando predicciones...\n","Precisión de KNN en clasificación de textos: 0.88\n","Ingrese la URL de un documento para clasificar: https://www.nasa.gov/news-release/nasa-sets-coverage-for-intuitive-machines-second-private-moon-landing/\n","El documento ingresado pertenece a la categoría: rec.sport.hockey\n","\n","Prueba con un documento del dataset:\n","Texto de prueba: \n","\n","And this is just the beginning. Fascist x-Soviet Armenian Government will \n","not get away with the genocide of 2.5 million Turks and Kurds, and 204,000 \n","Azeri people. Your criminal grandparents committed unheard-of crimes, \n","resorted to all conceivable methods of despotism, organized massacres, \n","poured petrol over babies and burned them, raped women and girls in front \n","of their parents who were bound hand and foot, took girls from their \n","mothers and fathers and appropriated personal property and ...\n","Categoría real: talk.politics.mideast\n","Categoría predicha: talk.politics.mideast\n"]}]}]}